{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b5aa4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Prior Art Search System üîç\n",
    "\n",
    "A comprehensive NLP-based system for automated prior art search and novelty assessment in patent applications.\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This system provides an end-to-end pipeline for:\n",
    "1. **Text Extraction**: Extract text from PDFs or accept direct input\n",
    "2. **Preprocessing**: Clean and normalize text using spaCy\n",
    "3. **Summarization**: Generate concise summaries using TextRank\n",
    "4. **Keyword Extraction**: Extract relevant keywords using YAKE, RAKE, and KeyBERT\n",
    "5. **Similarity Analysis**: Compare inventions with prior art using TF-IDF and BERT\n",
    "6. **Citation Ranking**: Rank prior art by relevance\n",
    "7. **Novelty Assessment**: Compute novelty scores\n",
    "\n",
    "## üèóÔ∏è Project Structure\n",
    "\n",
    "```\n",
    "prior_art_search/\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py          # Text cleaning and preprocessing\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ pdf_extractor.py          # PDF text extraction\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ summarization.py          # Text summarization\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ keyword_extraction.py     # Keyword extraction (YAKE, RAKE, KeyBERT)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ similarity_ranking.py     # Similarity computation and ranking\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py               # Main pipeline integrating all components\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ web_interface.py          # Flask web interface\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ input/                    # Input files directory\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ output/                   # Output results directory\n",
    "‚îú‚îÄ‚îÄ notebooks/                    # Jupyter notebooks for experimentation\n",
    "‚îú‚îÄ‚îÄ tests/                        # Unit tests\n",
    "‚îî‚îÄ‚îÄ requirements.txt              # Python dependencies\n",
    "```\n",
    "\n",
    "## üöÄ Installation\n",
    "\n",
    "### Step 1: Clone or Download the Project\n",
    "\n",
    "```bash\n",
    "cd prior_art_search\n",
    "```\n",
    "\n",
    "### Step 2: Create a Virtual Environment (Recommended)\n",
    "\n",
    "```bash\n",
    "# Create virtual environment\n",
    "python -m venv venv\n",
    "\n",
    "# Activate it\n",
    "# On Windows:\n",
    "venv\\Scripts\\activate\n",
    "# On Mac/Linux:\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "### Step 3: Install Dependencies\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Step 4: Download spaCy Language Model\n",
    "\n",
    "```bash\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "### Optional: Install sumy for Better Summarization\n",
    "\n",
    "```bash\n",
    "pip install sumy\n",
    "```\n",
    "\n",
    "## üìñ Usage\n",
    "\n",
    "### Option 1: Command Line (Python Script)\n",
    "\n",
    "```python\n",
    "from src.pipeline import PriorArtPipeline\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = PriorArtPipeline(output_dir=\"data/output\")\n",
    "\n",
    "# Your invention description\n",
    "invention = \"\"\"\n",
    "A novel machine learning system for real-time emotion detection\n",
    "using multimodal deep learning combining facial expressions and\n",
    "voice patterns...\n",
    "\"\"\"\n",
    "\n",
    "# Prior art documents\n",
    "prior_art = [\n",
    "    {\n",
    "        'text': 'Facial recognition system using CNNs...',\n",
    "        'metadata': {'id': 'P001', 'year': 2020}\n",
    "    },\n",
    "    # Add more documents...\n",
    "]\n",
    "\n",
    "# Run analysis\n",
    "results = pipeline.run_full_pipeline(\n",
    "    invention_input=invention,\n",
    "    prior_art_docs=prior_art,\n",
    "    is_file=False,\n",
    "    similarity_method=\"hybrid\",\n",
    "    save_results=True\n",
    ")\n",
    "\n",
    "# Print results\n",
    "pipeline.print_results(results)\n",
    "```\n",
    "\n",
    "### Option 2: Web Interface\n",
    "\n",
    "```bash\n",
    "cd src\n",
    "python web_interface.py\n",
    "```\n",
    "\n",
    "Then open your browser to `http://localhost:5000`\n",
    "\n",
    "### Option 3: Individual Modules\n",
    "\n",
    "#### Preprocessing\n",
    "```python\n",
    "from src.preprocessing import TextPreprocessor\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "cleaned = preprocessor.preprocess(text)\n",
    "```\n",
    "\n",
    "#### Keyword Extraction\n",
    "```python\n",
    "from src.keyword_extraction import KeywordExtractor\n",
    "\n",
    "extractor = KeywordExtractor()\n",
    "keywords = extractor.extract_with_yake(text, top_n=10)\n",
    "```\n",
    "\n",
    "#### Similarity Analysis\n",
    "```python\n",
    "from src.similarity_ranking import CitationRanker\n",
    "\n",
    "ranker = CitationRanker()\n",
    "ranked = ranker.rank_documents(query, documents, method=\"hybrid\")\n",
    "```\n",
    "\n",
    "## üîß Module Details\n",
    "\n",
    "### 1. Preprocessing (`preprocessing.py`)\n",
    "\n",
    "**Features:**\n",
    "- Text cleaning (remove URLs, emails, special characters)\n",
    "- Lemmatization\n",
    "- Stopword removal\n",
    "- Noun phrase extraction\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "preprocessor = TextPreprocessor()\n",
    "clean_text = preprocessor.clean_text(raw_text)\n",
    "lemmatized = preprocessor.lemmatize(text, remove_stopwords=True)\n",
    "```\n",
    "\n",
    "### 2. PDF Extraction (`pdf_extractor.py`)\n",
    "\n",
    "**Features:**\n",
    "- Extract text from PDFs using PyPDF2 or pdfplumber\n",
    "- Handle both file and direct text input\n",
    "- Extract metadata\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "extractor = PDFExtractor()\n",
    "result = extractor.extract(\"path/to/file.pdf\", method=\"pdfplumber\")\n",
    "print(result['text'])\n",
    "```\n",
    "\n",
    "### 3. Summarization (`summarization.py`)\n",
    "\n",
    "**Features:**\n",
    "- TextRank-based extractive summarization\n",
    "- Fallback to simple sentence extraction\n",
    "- Configurable summary length\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "summarizer = ModernTextSummarizer()\n",
    "summary = summarizer.summarize_with_textrank(text, sentence_count=3)\n",
    "```\n",
    "\n",
    "### 4. Keyword Extraction (`keyword_extraction.py`)\n",
    "\n",
    "**Algorithms:**\n",
    "- **YAKE**: Statistical + linguistic features\n",
    "- **RAKE**: Rapid keyword extraction\n",
    "- **KeyBERT**: BERT-based semantic extraction\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "extractor = KeywordExtractor()\n",
    "\n",
    "# Single method\n",
    "yake_kw = extractor.extract_with_yake(text, top_n=10)\n",
    "\n",
    "# All methods combined\n",
    "all_kw = extractor.extract_combined(text, top_n=10)\n",
    "unique = extractor.get_unique_keywords(all_kw, top_n=15)\n",
    "```\n",
    "\n",
    "### 5. Similarity & Ranking (`similarity_ranking.py`)\n",
    "\n",
    "**Methods:**\n",
    "- **TF-IDF**: Fast, interpretable, statistical\n",
    "- **BERT**: Semantic, context-aware\n",
    "- **Hybrid**: Combines both (recommended)\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "ranker = CitationRanker()\n",
    "\n",
    "# Rank documents\n",
    "ranked = ranker.rank_documents(\n",
    "    query_text=invention,\n",
    "    documents=prior_art_list,\n",
    "    method=\"hybrid\",\n",
    "    top_n=10\n",
    ")\n",
    "\n",
    "# Compute novelty\n",
    "novelty = ranker.compute_novelty_score(\n",
    "    query_text=invention,\n",
    "    prior_art_docs=prior_art_texts,\n",
    "    method=\"hybrid\"\n",
    ")\n",
    "```\n",
    "\n",
    "### 6. Main Pipeline (`pipeline.py`)\n",
    "\n",
    "**Complete Workflow:**\n",
    "```python\n",
    "pipeline = PriorArtPipeline()\n",
    "\n",
    "results = pipeline.run_full_pipeline(\n",
    "    invention_input=text_or_pdf_path,\n",
    "    prior_art_docs=prior_art_list,\n",
    "    is_file=False,\n",
    "    similarity_method=\"hybrid\",\n",
    "    save_results=True\n",
    ")\n",
    "```\n",
    "\n",
    "## üìä Output Format\n",
    "\n",
    "The pipeline generates a comprehensive JSON report:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"input_metadata\": {\n",
    "    \"filename\": \"invention.txt\",\n",
    "    \"num_pages\": 1,\n",
    "    \"input_length\": 1250\n",
    "  },\n",
    "  \"analysis\": {\n",
    "    \"original_text\": \"...\",\n",
    "    \"cleaned_text\": \"...\",\n",
    "    \"summary\": \"...\",\n",
    "    \"keywords\": {\n",
    "      \"yake\": [...],\n",
    "      \"rake\": [...],\n",
    "      \"unique_keywords\": [...]\n",
    "    }\n",
    "  },\n",
    "  \"prior_art_comparison\": {\n",
    "    \"ranked_citations\": [...],\n",
    "    \"novelty_metrics\": {\n",
    "      \"novelty_score\": 0.72,\n",
    "      \"max_similarity\": 0.28,\n",
    "      \"avg_similarity\": 0.15\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## üéØ Use Cases\n",
    "\n",
    "1. **Patent Filing**: Assess novelty before filing\n",
    "2. **Prior Art Search**: Find relevant existing patents\n",
    "3. **R&D**: Identify gaps in existing technology\n",
    "4. **Legal Analysis**: Support patent litigation\n",
    "5. **Technology Scouting**: Discover similar innovations\n",
    "\n",
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "### Similarity Methods\n",
    "\n",
    "- **TF-IDF**: Fastest, good for keyword matching\n",
    "- **BERT**: Best semantic understanding, slower\n",
    "- **Hybrid** (recommended): Balance of speed and accuracy\n",
    "\n",
    "### Keyword Extraction\n",
    "\n",
    "- **YAKE**: Best for general use, no training needed\n",
    "- **RAKE**: Fast, good for technical documents\n",
    "- **KeyBERT**: Most accurate, requires more resources\n",
    "\n",
    "## üìà Performance Tips\n",
    "\n",
    "1. **For faster processing**: Use `tfidf` similarity method\n",
    "2. **For better accuracy**: Use `hybrid` or `bert` method\n",
    "3. **Memory optimization**: Avoid loading KeyBERT unless needed\n",
    "4. **Large corpora**: Process in batches\n",
    "\n",
    "## üß™ Testing\n",
    "\n",
    "Run individual module tests:\n",
    "\n",
    "```bash\n",
    "cd src\n",
    "python preprocessing.py\n",
    "python keyword_extraction.py\n",
    "python similarity_ranking.py\n",
    "python pipeline.py\n",
    "```\n",
    "\n",
    "## üîç Example Workflow\n",
    "\n",
    "```python\n",
    "# Step 1: Initialize\n",
    "from src.pipeline import PriorArtPipeline\n",
    "pipeline = PriorArtPipeline()\n",
    "\n",
    "# Step 2: Prepare input\n",
    "invention = \"A neural network system for automated patent analysis...\"\n",
    "\n",
    "prior_art = [\n",
    "    {'text': 'Patent 1 description...', 'metadata': {'id': 'P1'}},\n",
    "    {'text': 'Patent 2 description...', 'metadata': {'id': 'P2'}},\n",
    "]\n",
    "\n",
    "# Step 3: Run analysis\n",
    "results = pipeline.run_full_pipeline(\n",
    "    invention_input=invention,\n",
    "    prior_art_docs=prior_art,\n",
    "    similarity_method=\"hybrid\"\n",
    ")\n",
    "\n",
    "# Step 4: View results\n",
    "pipeline.print_results(results)\n",
    "\n",
    "# Step 5: Check novelty\n",
    "novelty_score = results['prior_art_comparison']['novelty_metrics']['novelty_score']\n",
    "if novelty_score > 0.7:\n",
    "    print(\"HIGH novelty - proceed with patent filing\")\n",
    "elif novelty_score > 0.4:\n",
    "    print(\"MODERATE novelty - review similar patents\")\n",
    "else:\n",
    "    print(\"LOW novelty - significant prior art exists\")\n",
    "```\n",
    "\n",
    "## üìù Notes\n",
    "\n",
    "- **First Run**: May take longer as models are downloaded\n",
    "- **GPU**: Not required but speeds up BERT operations\n",
    "- **Memory**: Minimum 4GB RAM recommended\n",
    "- **Python**: Version 3.8+ required\n",
    "\n",
    "## ü§ù Contributing\n",
    "\n",
    "Feel free to enhance the system by:\n",
    "- Adding new keyword extraction methods\n",
    "- Implementing additional similarity metrics\n",
    "- Improving the web interface\n",
    "- Adding database integration\n",
    "\n",
    "## üìÑ License\n",
    "\n",
    "This project uses various open-source libraries. See individual module licenses.\n",
    "\n",
    "## üÜò Troubleshooting\n",
    "\n",
    "### Issue: spaCy model not found\n",
    "```bash\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "### Issue: KeyBERT slow on CPU\n",
    "Use TF-IDF method instead or install CUDA support\n",
    "\n",
    "### Issue: Import errors\n",
    "Make sure you're in the correct directory and virtual environment is activated\n",
    "\n",
    "## üìß Support\n",
    "\n",
    "For issues or questions, please refer to the documentation in each module file.\n",
    "\n",
    "---\n",
    "\n",
    "**Built with**: spaCy, Gensim, scikit-learn, YAKE, KeyBERT, Sentence-Transformers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
